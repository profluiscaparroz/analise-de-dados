# Regress√£o Lasso (L1 Regularization)

## Sum√°rio

1. [Introdu√ß√£o](#introdu√ß√£o)
2. [O Que √© Lasso?](#o-que-√©-lasso)
3. [Conceitos Fundamentais](#conceitos-fundamentais)
4. [Modelo Matem√°tico](#modelo-matem√°tico)
5. [Sele√ß√£o Autom√°tica de Vari√°veis](#sele√ß√£o-autom√°tica-de-vari√°veis)
6. [Par√¢metro de Regulariza√ß√£o (Œª ou Œ±)](#par√¢metro-de-regulariza√ß√£o-Œª-ou-Œ±)
7. [Vantagens e Desvantagens](#vantagens-e-desvantagens)
8. [Lasso vs Ridge vs OLS](#lasso-vs-ridge-vs-ols)
9. [Exemplos do Dia a Dia](#exemplos-do-dia-a-dia)
10. [Implementa√ß√£o em Python](#implementa√ß√£o-em-python)
11. [Elastic Net](#elastic-net)
12. [Casos Pr√°ticos](#casos-pr√°ticos)
13. [Limita√ß√µes e Cuidados](#limita√ß√µes-e-cuidados)
14. [Refer√™ncias](#refer√™ncias)

---

## Introdu√ß√£o

A **Regress√£o Lasso** (*Least Absolute Shrinkage and Selection Operator*) √© uma t√©cnica de regulariza√ß√£o que, al√©m de encolher coeficientes como o Ridge, pode **zer√°-los completamente**, realizando **sele√ß√£o autom√°tica de vari√°veis**.

### Por que Lasso √© Importante?

Em muitos problemas do mundo real:
- Temos **centenas ou milhares** de vari√°veis potenciais
- Apenas **algumas s√£o realmente importantes**
- Precisamos **identificar** quais vari√°veis usar
- Queremos um modelo **simples e interpret√°vel**

Lasso resolve todos esses problemas!

### Diferen√ßa Chave: Ridge vs Lasso

```
Ridge (L2):
‚Ä¢ Encolhe coeficientes ‚Üí pr√≥ximos de zero
‚Ä¢ NUNCA zera coeficientes
‚Ä¢ Mant√©m todas as vari√°veis

Lasso (L1):
‚Ä¢ Encolhe coeficientes ‚Üí pr√≥ximos de zero
‚Ä¢ PODE zerar coeficientes ‚úì
‚Ä¢ Remove vari√°veis n√£o importantes
```

### Aplica√ß√µes Principais

‚úÖ **Gen√¥mica**: Identificar genes relevantes entre milhares  
‚úÖ **Text Mining**: Selecionar palavras importantes  
‚úÖ **Finance**: Escolher indicadores econ√¥micos  
‚úÖ **Marketing**: Identificar canais efetivos  
‚úÖ **Medicina**: Descobrir fatores de risco principais

---

## O Que √© Lasso?

### LASSO = Least Absolute Shrinkage and Selection Operator

**Proposto por:** Robert Tibshirani (1996)

**Objetivo Duplo:**
1. **Shrinkage** (Encolhimento) - Regulariza√ß√£o como Ridge
2. **Selection** (Sele√ß√£o) - Zera coeficientes de vari√°veis n√£o importantes

### Intui√ß√£o Geom√©trica

**Ridge (penaliza√ß√£o L2):**
- Restringe coeficientes a uma **esfera**
- Coeficientes raramente s√£o exatamente zero
- Todos permanecem no modelo

**Lasso (penaliza√ß√£o L1):**
- Restringe coeficientes a um **diamante** (ou losango)
- Coeficientes frequentemente tocam os **v√©rtices** (onde alguns s√£o zero)
- Vari√°veis s√£o removidas automaticamente

### Visualiza√ß√£o 2D

```
Ridge (c√≠rculo):              Lasso (diamante):
      |                             /\
   ‚óè  |  ‚óè                         /  \
  ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ  ‚Üê sempre interior   ‚óè ‚îÄ‚îÄ ‚óè ‚Üê toca v√©rtices!
   ‚óè  |  ‚óè                         \  /
      |                             \/
      
Solu√ß√£o OLS: ‚óè                V√©rtice = 1 coef. √© ZERO
```

---

## Conceitos Fundamentais

### Regulariza√ß√£o L1

**Defini√ß√£o:** Penaliza√ß√£o pela **soma dos valores absolutos** dos coeficientes.

$$\text{Penaliza√ß√£o L1} = \lambda \sum_{j=1}^{p}|\beta_j|$$

**Caracter√≠sticas:**
- N√£o diferenci√°vel em Œ≤ = 0
- Promove **esparsidade** (muitos coeficientes zero)
- Efetua sele√ß√£o de vari√°veis

### Esparsidade (Sparsity)

**Esparsidade** = Muitos coeficientes s√£o exatamente zero

**Exemplo:**
```
Modelo com 100 vari√°veis:
OLS:    100 coeficientes n√£o-zero
Ridge:  100 coeficientes pequenos (mas n√£o-zero)
Lasso:  15 coeficientes n√£o-zero, 85 zeros ‚Üê ESPARSO
```

**Benef√≠cios:**
‚úÖ Modelo mais simples  
‚úÖ Mais interpret√°vel  
‚úÖ Menos overfitting  
‚úÖ Reduz custos (menos vari√°veis para coletar)

### Sele√ß√£o de Vari√°veis

Lasso faz sele√ß√£o **automaticamente** como parte da otimiza√ß√£o:
- **N√£o requer** m√©todos stepwise
- **N√£o requer** decis√µes ad-hoc
- **N√£o requer** m√∫ltiplos ajustes de modelos

**Exemplo:**
```python
# Lasso com Œ±=0.1
modelo = Lasso(alpha=0.1)
modelo.fit(X, y)

# Vari√°veis selecionadas automaticamente
vars_selecionadas = X.columns[modelo.coef_ != 0]
print(f"Selecionadas: {list(vars_selecionadas)}")
# Output: ['Area', 'Quartos', 'Localizacao']
# Idade, Distancia, Andar foram REMOVIDAS (coef = 0)
```

---

## Modelo Matem√°tico

### Fun√ß√£o Objetivo

Lasso minimiza:

$$\hat{\beta}^{lasso} = \arg\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p}|\beta_j| \right\}$$

Ou em nota√ß√£o compacta:

$$\hat{\beta}^{lasso} = \arg\min_{\beta} \left\{ ||\mathbf{y} - \mathbf{X}\beta||_2^2 + \lambda ||\beta||_1 \right\}$$

Onde:
- **||y - XŒ≤||‚ÇÇ¬≤** = Soma dos quadrados dos res√≠duos (RSS)
- **||Œ≤||‚ÇÅ** = Norma L1 = Œ£|Œ≤‚±º| (soma dos valores absolutos)
- **Œª** = Par√¢metro de regulariza√ß√£o (Œª ‚â• 0)

### Forma de Otimiza√ß√£o Restrita

Equivalentemente, Lasso pode ser expresso como:

$$\text{minimize} \quad \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$
$$\text{sujeito a} \quad \sum_{j=1}^{p}|\beta_j| \leq t$$

Onde **t** controla o n√≠vel de regulariza√ß√£o.

### Diferen√ßa nas Penaliza√ß√µes

| M√©todo | Penaliza√ß√£o | Forma Geom√©trica | Esparsidade |
|--------|-------------|------------------|-------------|
| **OLS** | Nenhuma | - | N√£o |
| **Ridge** | Œ£Œ≤‚±º¬≤ (L2) | Esfera | N√£o |
| **Lasso** | Œ£\|Œ≤‚±º\| (L1) | Diamante | **Sim** |

### Por que L1 Gera Esparsidade?

**Geometricamente:**
- Regi√£o restrita de Lasso tem **v√©rtices**
- Elipses de RSS frequentemente tocam os **v√©rtices**
- Nos v√©rtices, pelo menos um coeficiente √© zero

**Algebricamente:**
- Derivada de |Œ≤| n√£o existe em Œ≤=0
- Algoritmo de otimiza√ß√£o "empurra" coeficientes pequenos para exatamente zero

### N√£o H√° Solu√ß√£o Fechada

Diferente de Ridge, Lasso **n√£o tem solu√ß√£o anal√≠tica**.

**Algoritmos usados:**
- **Coordinate Descent** (mais comum)
- **LARS** (Least Angle Regression)
- **Proximal Gradient Descent**

---

## Sele√ß√£o Autom√°tica de Vari√°veis

### Como Lasso Seleciona?

√Ä medida que Œª aumenta:

```
Œª = 0:    Todos coeficientes ‚â† 0 (igual OLS)
Œª pequeno: Alguns coeficientes ‚Üí 0
Œª m√©dio:   Muitos coeficientes = 0
Œª grande:  Quase todos = 0
Œª ‚Üí ‚àû:    Todos coeficientes = 0
```

### Ordem de Remo√ß√£o

Vari√°veis s√£o removidas em **ordem de import√¢ncia**:
1. Primeiro: vari√°veis menos correlacionadas com Y
2. √öltimo: vari√°veis mais importantes

### Lasso Path

Visualiza√ß√£o de como coeficientes evoluem com Œª:

```python
from sklearn.linear_model import lasso_path

alphas, coefs, _ = lasso_path(X, y, alphas=alphas_range)

plt.plot(alphas, coefs.T)
plt.xscale('log')
plt.xlabel('Œ±')
plt.ylabel('Coeficientes')
plt.title('Lasso Path')
```

**Interpreta√ß√£o:**
- Cada linha = uma vari√°vel
- Linhas que tocam zero = vari√°veis removidas
- Ordem de chegada ao zero = ordem de import√¢ncia (inversa)

### Vantagens da Sele√ß√£o Autom√°tica

‚úÖ **Objetiva** - Baseada em dados, n√£o em julgamento subjetivo  
‚úÖ **Eficiente** - Um √∫nico ajuste do modelo  
‚úÖ **Reproduz√≠vel** - Sempre mesmo resultado  
‚úÖ **Integrada** - Regulariza√ß√£o e sele√ß√£o juntas

### Limita√ß√µes da Sele√ß√£o Lasso

‚ö†Ô∏è **Vari√°veis correlacionadas**: Pode remover vari√°veis importantes se correlacionadas  
‚ö†Ô∏è **Instabilidade**: Pequenas mudan√ßas nos dados podem mudar sele√ß√£o  
‚ö†Ô∏è **p > n**: Seleciona no m√°ximo n vari√°veis (mesmo que mais sejam relevantes)  
‚ö†Ô∏è **Agrupamentos**: N√£o trata bem grupos de vari√°veis correlacionadas

---

## Par√¢metro de Regulariza√ß√£o (Œª ou Œ±)

### Nota√ß√£o

- **Œª** (lambda): Nota√ß√£o estat√≠stica
- **Œ±** (alpha): scikit-learn

S√£o equivalentes: Œ± = Œª

### Impacto de Œª/Œ±

**Œ± = 0:**
```
‚Ä¢ Sem regulariza√ß√£o
‚Ä¢ Equivalente a OLS
‚Ä¢ Todas vari√°veis no modelo
```

**Œ± pequeno (ex: 0.001):**
```
‚Ä¢ Regulariza√ß√£o fraca
‚Ä¢ Poucas vari√°veis removidas
‚Ä¢ Coeficientes pr√≥ximos de OLS
```

**Œ± √≥timo (ex: 0.1-1):**
```
‚Ä¢ Equil√≠brio vi√©s-vari√¢ncia
‚Ä¢ Sele√ß√£o razo√°vel de vari√°veis
‚Ä¢ Melhor generaliza√ß√£o
```

**Œ± grande (ex: 100):**
```
‚Ä¢ Regulariza√ß√£o forte
‚Ä¢ Quase todas vari√°veis removidas
‚Ä¢ Underfitting
```

**Œ± ‚Üí ‚àû:**
```
‚Ä¢ Todas vari√°veis removidas
‚Ä¢ Modelo apenas com intercepto
‚Ä¢ Prediz m√©dia de Y
```

### Escolha de Œ±

#### 1. Valida√ß√£o Cruzada (Recomendado)

```python
from sklearn.linear_model import LassoCV

lasso_cv = LassoCV(cv=5, alphas=np.logspace(-4, 1, 100))
lasso_cv.fit(X, y)

print(f"Melhor Œ±: {lasso_cv.alpha_}")
print(f"Vari√°veis selecionadas: {np.sum(lasso_cv.coef_ != 0)}")
```

#### 2. Crit√©rio de Informa√ß√£o

AIC, BIC ou crit√©rios personalizados.

#### 3. An√°lise Visual

Plotar Lasso Path e escolher Œ± que d√° conjunto razo√°vel de vari√°veis.

---

## Vantagens e Desvantagens

### Vantagens ‚úÖ

**1. Sele√ß√£o Autom√°tica de Vari√°veis**
- Remove vari√°veis n√£o importantes
- Modelo mais simples e interpret√°vel
- √önico grande diferencial sobre Ridge!

**2. Reduz Overfitting**
- Menos vari√°veis = menos complexidade
- Melhor generaliza√ß√£o

**3. Interpretabilidade**
- Modelo esparso √© mais f√°cil de explicar
- Identifica fatores realmente importantes

**4. Reduz Custo**
- Menos vari√°veis para coletar
- Menos processamento
- Mais eficiente em produ√ß√£o

**5. Funciona com p > n**
- Como Ridge, funciona mesmo com mais vari√°veis que observa√ß√µes

**6. Robustez**
- Remove vari√°veis ruidosas automaticamente

### Desvantagens ‚ùå

**1. Limita√ß√£o com Vari√°veis Correlacionadas**
- Tende a selecionar apenas uma de um grupo correlacionado
- Pode remover vari√°vel importante se correlacionada

**Exemplo:**
```
Altura e Peso s√£o correlacionados
Lasso pode:
  - Manter Altura, remover Peso, OU
  - Manter Peso, remover Altura
Ambos igualmente importantes, mas Lasso escolhe um arbitrariamente
```

**2. Instabilidade na Sele√ß√£o**
- Pequenas mudan√ßas nos dados podem mudar sele√ß√£o
- Menos est√°vel que Ridge

**3. Limita√ß√£o p > n**
- Seleciona no m√°ximo n vari√°veis
- Mesmo que mais sejam relevantes

**4. N√£o Diferenci√°vel**
- Otimiza√ß√£o mais complexa que Ridge
- Algoritmos mais lentos

**5. Pode Ser Muito Agressivo**
- Pode remover vari√°veis √∫teis
- Œ± errado pode prejudicar muito

**6. Coeficientes Enviesados**
- Como Ridge, adiciona vi√©s
- Trade-off vi√©s-vari√¢ncia

---

## Lasso vs Ridge vs OLS

### Tabela Comparativa

| Aspecto | OLS | Ridge (L2) | Lasso (L1) |
|---------|-----|------------|------------|
| **Penaliza√ß√£o** | Nenhuma | Œ£Œ≤‚±º¬≤ | Œ£\|Œ≤‚±º\| |
| **Esparsidade** | N√£o | N√£o | **Sim** |
| **Sele√ß√£o de vari√°veis** | N√£o | N√£o | **Sim** |
| **Multicolinearidade** | Problem√°tica | Resolve | Parcial |
| **Interpretabilidade** | Alta | Baixa | **Alta** |
| **Estabilidade** | Baixa | Alta | **Moderada** |
| **p > n** | N√£o funciona | Funciona | Funciona |
| **Solu√ß√£o** | Anal√≠tica | Anal√≠tica | **Num√©rica** |
| **Velocidade** | R√°pida | R√°pida | **Moderada** |
| **Quando usar** | p < n, sem multicolinearidade | Multicolinearidade, todas vars importantes | **Sele√ß√£o de vari√°veis importante** |

### Quando Usar Cada Um?

**Use OLS quando:**
- p << n (poucas vari√°veis, muitas observa√ß√µes)
- Sem multicolinearidade
- Todas vari√°veis s√£o teoricamente importantes

**Use Ridge quando:**
- Multicolinearidade severa
- Todas vari√°veis devem permanecer no modelo
- p > n
- Estabilidade √© crucial

**Use Lasso quando:**
- Muitas vari√°veis, poucas importantes
- Quer identificar quais vari√°veis importam
- Interpretabilidade √© crucial
- Modelo esparso √© desej√°vel

**Use Elastic Net quando:**
- Vari√°veis correlacionadas E quer sele√ß√£o
- Melhor dos dois mundos
- Combina L1 e L2

### Regra Pr√°tica

```
Decis√£o r√°pida:
‚îú‚îÄ Precisa selecionar vari√°veis? 
‚îÇ  ‚îú‚îÄ Sim ‚Üí Lasso ou Elastic Net
‚îÇ  ‚îî‚îÄ N√£o ‚Üí Ridge
‚îî‚îÄ Poucas vari√°veis, sem multicolinearidade?
   ‚îî‚îÄ Sim ‚Üí OLS
```

---

## Exemplos do Dia a Dia

### Exemplo 1: Previs√£o de Doen√ßa com Dados Gen√©ticos

**Contexto:** Pesquisa com 10.000 genes, mas apenas ~100 realmente relacionados √† doen√ßa.

**Problema com OLS/Ridge:**
- OLS: Imposs√≠vel (p >> n)
- Ridge: Funciona, mas mant√©m 10.000 genes

**Solu√ß√£o com Lasso:**
```python
lasso = LassoCV(cv=5)
lasso.fit(X_genes, y_doenca)

# Lasso seleciona apenas 127 genes
genes_selecionados = genes[lasso.coef_ != 0]
print(f"Genes selecionados: {len(genes_selecionados)}")
# Output: 127 (de 10.000!)
```

**Vantagens:**
- Modelo simples (127 vs 10.000)
- Identificou genes relevantes
- Interpret√°vel para m√©dicos
- Base para pesquisa adicional

### Exemplo 2: Text Mining - Classifica√ß√£o de Sentimentos

**Contexto:** Classificar reviews como positivos/negativos. 5.000 palavras √∫nicas, mas poucas s√£o discriminativas.

**Problema:**
- A maioria das palavras n√£o importa
- "a", "o", "de" n√£o ajudam
- Apenas palavras como "excelente", "p√©ssimo", "adorei" importam

**Lasso:**
```python
# Vetoriza√ß√£o TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_words = vectorizer.fit_transform(reviews)

# Lasso
lasso = Lasso(alpha=0.01)
lasso.fit(X_words, sentiment)

# Palavras importantes
palavras = vectorizer.get_feature_names_out()
importantes = palavras[lasso.coef_ != 0]
print(f"Palavras selecionadas: {len(importantes)} de 5000")
print(importantes[:10])
# Output: ['excelente', '√≥timo', 'p√©ssimo', 'horr√≠vel', ...]
```

### Exemplo 3: Marketing Mix - Identificar Canais Efetivos

**Contexto:** Empresa investe em 20 canais de marketing, mas nem todos s√£o efetivos.

**Vari√°veis:**
- TV (5 tipos), R√°dio (3 tipos), Outdoor, Online (Google, Facebook, Instagram, YouTube, LinkedIn, Twitter), Email, SMS, etc.

**Lasso:**
```python
lasso = LassoCV(cv=5)
lasso.fit(X_canais, vendas)

# Canais efetivos
canais_efetivos = canais[lasso.coef_ != 0]
print(f"Canais efetivos: {list(canais_efetivos)}")
# Output: ['TV_Prime', 'Google_Ads', 'Facebook', 'Email']

# Realocar or√ßamento
# Parar de gastar em canais com coef = 0
# Focar em canais selecionados
```

**Resultado:**
- Identificou 4 canais principais
- Cortou 16 canais ineficazes
- Realocou or√ßamento
- Aumento de 25% no ROI

### Exemplo 4: Previs√£o de Pre√ßo de Im√≥veis com Muitas Features

**Contexto:** Sistema autom√°tico com 100+ caracter√≠sticas potenciais.

**Features:**
- √Årea, quartos, banheiros, idade, andar
- Dist√¢ncia a: 20 tipos de POIs (escolas, hospitais, metr√¥, parques, shopping, etc.)
- Caracter√≠sticas do bairro: 30 vari√°veis demogr√°ficas
- Caracter√≠sticas da regi√£o: 25 vari√°veis

**Lasso:**
```python
lasso_cv = LassoCV(cv=10)
lasso_cv.fit(X_todas, preco)

# Lasso seleciona 18 de 100+ features
features_importantes = features[lasso_cv.coef_ != 0]

print("Features mais importantes:")
for feat in features_importantes:
    print(f"  {feat}: {lasso_cv.coef_[features.get_loc(feat)]:.2f}")
    
# Output:
#   Area: 2500.32
#   Quartos: 18000.15
#   Dist_Metro: -3200.45
#   Dist_Shopping: -1500.22
#   ...
```

**Aplica√ß√£o Pr√°tica:**
- Sistema s√≥ coleta 18 features (n√£o 100+)
- Avalia√ß√£o mais r√°pida
- Menor custo
- Mesma precis√£o

### Exemplo 5: Previs√£o de S√©ries Temporais com Features Defasadas

**Contexto:** Prever vendas usando 50 lags (vendas dos √∫ltimos 50 dias).

**Problema:**
- Lags consecutivos s√£o correlacionados
- Nem todos os lags s√£o importantes
- Alguns per√≠odos (7, 14, 30 dias) mais relevantes

**Lasso:**
```python
# Criar features defasadas
for i in range(1, 51):
    df[f'lag_{i}'] = df['vendas'].shift(i)

X_lags = df[[f'lag_{i}' for i in range(1, 51)]]

lasso = LassoCV(cv=5)
lasso.fit(X_lags, y)

# Lags selecionados
lags_importantes = [i for i in range(1, 51) 
                    if lasso.coef_[i-1] != 0]
print(f"Lags selecionados: {lags_importantes}")
# Output: [1, 2, 7, 14, 30] (exemplo)
```

**Insights:**
- Vendas de ontem e anteontem importam
- Efeito semanal (lag 7)
- Efeito quinzenal (lag 14)
- Efeito mensal (lag 30)

---

## Implementa√ß√£o em Python

### Exemplo Completo: Sele√ß√£o de Vari√°veis em Dados de Im√≥veis

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import Lasso, LassoCV, lasso_path
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# Configurar visualiza√ß√µes
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# ==============================================
# 1. GERAR DADOS COM MUITAS VARI√ÅVEIS
# ==============================================
np.random.seed(42)
n = 300  # observa√ß√µes

# Vari√°veis IMPORTANTES (realmente afetam o pre√ßo)
area = np.random.normal(80, 20, n)
quartos = np.random.poisson(3, n)
localizacao = np.random.choice([0, 1, 2], n, p=[0.3, 0.5, 0.2])

# Vari√°veis MENOS IMPORTANTES
idade = np.random.exponential(10, n)
andar = np.random.randint(1, 20, n)

# Vari√°veis IRRELEVANTES (ru√≠do)
# Dist√¢ncias a 10 POIs diferentes
pois = []
for i in range(10):
    pois.append(np.random.uniform(0, 20, n))

# Pre√ßo baseado principalmente em Area, Quartos, Localizacao
preco = (150000 + 
         2800 * area +              # IMPORTANTE
         25000 * quartos +           # IMPORTANTE
         50000 * localizacao +       # IMPORTANTE
         -500 * idade +              # MENOS IMPORTANTE
         800 * andar +               # MENOS IMPORTANTE
         np.random.normal(0, 25000, n))

# Criar DataFrame
df = pd.DataFrame({
    'Preco': preco,
    'Area': area,
    'Quartos': quartos,
    'Localizacao': localizacao,
    'Idade': idade,
    'Andar': andar
})

# Adicionar POIs (vari√°veis irrelevantes)
for i, poi in enumerate(pois, 1):
    df[f'Dist_POI_{i}'] = poi

print("=" * 70)
print("LASSO: SELE√á√ÉO AUTOM√ÅTICA DE VARI√ÅVEIS")
print("=" * 70)
print(f"\nN√∫mero de observa√ß√µes: {n}")
print(f"N√∫mero de vari√°veis: {df.shape[1] - 1}")
print(f"\nVari√°veis realmente importantes: Area, Quartos, Localizacao")
print(f"Vari√°veis menos importantes: Idade, Andar")
print(f"Vari√°veis irrelevantes: Dist_POI_1 a Dist_POI_10")

# ==============================================
# 2. PREPARAR DADOS
# ==============================================

X = df.drop('Preco', axis=1)
y = df['Preco']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Padronizar
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ==============================================
# 3. COMPARAR OLS, RIDGE, LASSO
# ==============================================

print("\n" + "=" * 70)
print("COMPARA√á√ÉO: OLS vs RIDGE vs LASSO")
print("=" * 70)

# OLS
ols = LinearRegression()
ols.fit(X_train_scaled, y_train)
y_pred_ols = ols.predict(X_test_scaled)
r2_ols = r2_score(y_test, y_pred_ols)
vars_ols = np.sum(ols.coef_ != 0)

# Ridge
ridge = Ridge(alpha=10)
ridge.fit(X_train_scaled, y_train)
y_pred_ridge = ridge.predict(X_test_scaled)
r2_ridge = r2_score(y_test, y_pred_ridge)
vars_ridge = np.sum(np.abs(ridge.coef_) > 0.01)  # Aproxima√ß√£o

# Lasso
lasso = Lasso(alpha=1000)
lasso.fit(X_train_scaled, y_train)
y_pred_lasso = lasso.predict(X_test_scaled)
r2_lasso = r2_score(y_test, y_pred_lasso)
vars_lasso = np.sum(lasso.coef_ != 0)

# Tabela de compara√ß√£o
print("\n{:<20} {:>10} {:>10} {:>10}".format(
    "M√©trica", "OLS", "Ridge", "Lasso"
))
print("-" * 55)
print("{:<20} {:>10.4f} {:>10.4f} {:>10.4f}".format(
    "R¬≤ Teste", r2_ols, r2_ridge, r2_lasso
))
print("{:<20} {:>10} {:>10} {:>10}".format(
    "Vars no modelo", vars_ols, vars_ridge, vars_lasso
))

print(f"\nüéØ Lasso selecionou apenas {vars_lasso} de {X.shape[1]} vari√°veis!")

# ==============================================
# 4. IDENTIFICAR VARI√ÅVEIS SELECIONADAS
# ==============================================

print("\n" + "=" * 70)
print("VARI√ÅVEIS SELECIONADAS PELO LASSO")
print("=" * 70)

vars_selecionadas = X.columns[lasso.coef_ != 0]
vars_removidas = X.columns[lasso.coef_ == 0]

print(f"\n‚úì SELECIONADAS ({len(vars_selecionadas)}):")
for var in vars_selecionadas:
    coef = lasso.coef_[X.columns.get_loc(var)]
    print(f"  {var:20s}: {coef:>12,.2f}")

print(f"\n‚úó REMOVIDAS ({len(vars_removidas)}):")
for var in vars_removidas:
    print(f"  {var}")

# ==============================================
# 5. ENCONTRAR Œ± √ìTIMO COM VALIDA√á√ÉO CRUZADA
# ==============================================

print("\n" + "=" * 70)
print("BUSCA DO MELHOR Œ± (VALIDA√á√ÉO CRUZADA)")
print("=" * 70)

lasso_cv = LassoCV(cv=5, alphas=np.logspace(-1, 4, 100), max_iter=10000)
lasso_cv.fit(X_train_scaled, y_train)

print(f"\nMelhor Œ±: {lasso_cv.alpha_:.2f}")

y_pred_cv = lasso_cv.predict(X_test_scaled)
r2_cv = r2_score(y_test, y_pred_cv)
vars_cv = np.sum(lasso_cv.coef_ != 0)

print(f"R¬≤ no teste: {r2_cv:.4f}")
print(f"Vari√°veis selecionadas: {vars_cv}")

print("\nVari√°veis selecionadas pelo modelo √≥timo:")
vars_sel_cv = X.columns[lasso_cv.coef_ != 0]
for var in vars_sel_cv:
    coef = lasso_cv.coef_[X.columns.get_loc(var)]
    print(f"  {var:20s}: {coef:>12,.2f}")

# ==============================================
# 6. LASSO PATH (trajet√≥ria dos coeficientes)
# ==============================================

print("\n" + "=" * 70)
print("LASSO PATH: Evolu√ß√£o dos Coeficientes")
print("=" * 70)

alphas_path = np.logspace(-1, 4, 100)
coefs_lasso, _, _ = lasso_path(X_train_scaled, y_train, 
                                alphas=alphas_path, max_iter=10000)

plt.figure(figsize=(12, 6))
for i, var in enumerate(X.columns):
    # Destacar vari√°veis importantes
    if var in ['Area', 'Quartos', 'Localizacao']:
        plt.plot(alphas_path, coefs_lasso[i, :], 
                linewidth=2.5, label=var)
    else:
        plt.plot(alphas_path, coefs_lasso[i, :], 
                linewidth=0.8, alpha=0.5, color='gray')

plt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', 
            linewidth=2, label=f'Œ± √≥timo = {lasso_cv.alpha_:.1f}')
plt.xscale('log')
plt.xlabel('Œ± (escala log)', fontsize=12)
plt.ylabel('Coeficientes', fontsize=12)
plt.title('Lasso Path: Como Vari√°veis S√£o Removidas', 
          fontsize=14, fontweight='bold')
plt.legend(loc='best', fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("\nObserva√ß√µes:")
print("‚Ä¢ Linhas grossas: vari√°veis importantes (Area, Quartos, Localizacao)")
print("‚Ä¢ Linhas finas: vari√°veis menos importantes ou irrelevantes")
print("‚Ä¢ Quanto maior Œ±, mais vari√°veis v√£o para zero")
print(f"‚Ä¢ No Œ± √≥timo ({lasso_cv.alpha_:.1f}), {vars_cv} vari√°veis permanecem")

# ==============================================
# 7. COMPARAR COEFICIENTES
# ==============================================

fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# OLS
axes[0].barh(range(len(X.columns)), ols.coef_, color='steelblue', edgecolor='black')
axes[0].set_yticks(range(len(X.columns)))
axes[0].set_yticklabels(X.columns, fontsize=9)
axes[0].axvline(x=0, color='red', linestyle='--', linewidth=1)
axes[0].set_xlabel('Coeficiente', fontsize=11)
axes[0].set_title(f'OLS\n({vars_ols} vari√°veis)', fontsize=12, fontweight='bold')
axes[0].grid(True, alpha=0.3, axis='x')

# Ridge
axes[1].barh(range(len(X.columns)), ridge.coef_, color='darkgreen', edgecolor='black')
axes[1].set_yticks(range(len(X.columns)))
axes[1].set_yticklabels(X.columns, fontsize=9)
axes[1].axvline(x=0, color='red', linestyle='--', linewidth=1)
axes[1].set_xlabel('Coeficiente', fontsize=11)
axes[1].set_title(f'Ridge (Œ±=10)\n(~{vars_ridge} vari√°veis)', 
                   fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='x')

# Lasso
axes[2].barh(range(len(X.columns)), lasso_cv.coef_, color='darkred', edgecolor='black')
axes[2].set_yticks(range(len(X.columns)))
axes[2].set_yticklabels(X.columns, fontsize=9)
axes[2].axvline(x=0, color='red', linestyle='--', linewidth=1)
axes[2].set_xlabel('Coeficiente', fontsize=11)
axes[2].set_title(f'Lasso (Œ±={lasso_cv.alpha_:.1f})\n({vars_cv} vari√°veis)', 
                   fontsize=12, fontweight='bold')
axes[2].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# ==============================================
# 8. AN√ÅLISE DE IMPORT√ÇNCIA
# ==============================================

print("\n" + "=" * 70)
print("AN√ÅLISE DE IMPORT√ÇNCIA")
print("=" * 70)

importancia = pd.DataFrame({
    'Variavel': X.columns,
    'Coef_Lasso': lasso_cv.coef_,
    'Abs_Coef': np.abs(lasso_cv.coef_)
}).sort_values('Abs_Coef', ascending=False)

print("\nRanking de Import√¢ncia (Lasso):")
print(importancia[importancia['Coef_Lasso'] != 0].to_string(index=False))

# Visualizar top 10
top_10 = importancia.head(10)
plt.figure(figsize=(10, 6))
colors = ['darkred' if c != 0 else 'lightgray' for c in top_10['Coef_Lasso']]
plt.barh(top_10['Variavel'], top_10['Abs_Coef'], color=colors, edgecolor='black')
plt.xlabel('Import√¢ncia Absoluta |Coeficiente|', fontsize=12)
plt.title('Top 10 Vari√°veis por Import√¢ncia (Lasso)', 
          fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print("\n" + "=" * 70)
print("AN√ÅLISE COMPLETA FINALIZADA")
print("=" * 70)

# Conclus√£o
print("\nüìä RESUMO:")
print(f"  ‚Ä¢ Dataset tinha {X.shape[1]} vari√°veis")
print(f"  ‚Ä¢ Lasso selecionou {vars_cv} vari√°veis")
print(f"  ‚Ä¢ Removeu {X.shape[1] - vars_cv} vari√°veis irrelevantes")
print(f"  ‚Ä¢ R¬≤ mantido em {r2_cv:.4f}")
print(f"  ‚Ä¢ Modelo mais simples e interpret√°vel!")
```

---

## Elastic Net

### O Que √© Elastic Net?

**Elastic Net** combina as penaliza√ß√µes L1 (Lasso) e L2 (Ridge):

$$\text{minimize} \quad RSS + \lambda_1 \sum_{j}|\beta_j| + \lambda_2 \sum_{j}\beta_j^2$$

Ou:

$$\text{minimize} \quad RSS + \lambda \left( \alpha ||\beta||_1 + \frac{1-\alpha}{2} ||\beta||_2^2 \right)$$

Onde **Œ± ‚àà [0, 1]** controla o mix:
- **Œ± = 1**: Lasso puro
- **Œ± = 0**: Ridge puro
- **Œ± = 0.5**: Meio a meio

### Vantagens do Elastic Net

‚úÖ **Melhor que Lasso** quando vari√°veis s√£o correlacionadas  
‚úÖ **Sele√ß√£o de grupos** - Seleciona/remove grupos de vari√°veis correlacionadas juntas  
‚úÖ **Mais est√°vel** que Lasso  
‚úÖ **Funciona com p > n** como Lasso e Ridge

### Quando Usar Elastic Net?

Use quando:
- Vari√°veis s√£o correlacionadas E quer sele√ß√£o
- Lasso √© inst√°vel nos seus dados
- Quer o "melhor dos dois mundos"

### Implementa√ß√£o

```python
from sklearn.linear_model import ElasticNetCV

elastic = ElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99], cv=5)
elastic.fit(X_scaled, y)

print(f"Œ± √≥timo: {elastic.alpha_}")
print(f"l1_ratio √≥timo: {elastic.l1_ratio_}")
print(f"Vari√°veis selecionadas: {np.sum(elastic.coef_ != 0)}")
```

---

## Casos Pr√°ticos

### Caso 1: Identifica√ß√£o de Genes em Pesquisa M√©dica

**Resultado:** De 20.000 genes, Lasso identificou 85 como relevantes para a doen√ßa.

### Caso 2: Otimiza√ß√£o de Campanhas de Marketing

**Resultado:** De 30 canais, Lasso identificou 7 canais realmente efetivos. Empresa realocou or√ßamento e aumentou ROI em 30%.

---

## Limita√ß√µes e Cuidados

### 1. Vari√°veis Correlacionadas

Lasso pode remover vari√°veis importantes se correlacionadas.

**Solu√ß√£o:** Use Elastic Net.

### 2. Instabilidade

Pequenas mudan√ßas nos dados podem mudar sele√ß√£o drasticamente.

**Solu√ß√£o:** 
- Use Elastic Net
- Bootstrap/cross-validation repetidos
- An√°lise de estabilidade da sele√ß√£o

### 3. Limita√ß√£o p > n

Lasso seleciona no m√°ximo n vari√°veis.

**Solu√ß√£o:** Se espera mais vari√°veis importantes, considere outros m√©todos.

### 4. Vi√©s nos Coeficientes

Coeficientes selecionados s√£o enviesados (shrunken).

**Solu√ß√£o:** Ap√≥s sele√ß√£o, reajustar OLS apenas com vari√°veis selecionadas.

### 5. Hiperpar√¢metro Œ±

Escolha errada de Œ± pode prejudicar muito.

**Solu√ß√£o:** Sempre use valida√ß√£o cruzada.

---

## Refer√™ncias

### Artigo Original

**TIBSHIRANI, Robert.** "Regression Shrinkage and Selection via the Lasso." *Journal of the Royal Statistical Society. Series B*, v. 58, n. 1, p. 267-288, 1996.
- Artigo que introduziu Lasso

### Livros

**HASTIE, Trevor; TIBSHIRANI, Robert; FRIEDMAN, Jerome.** *The Elements of Statistical Learning*. 2. ed. Springer, 2009.
- Cap√≠tulo 3.4: Lasso e Elastic Net

**JAMES, Gareth et al.** *An Introduction to Statistical Learning*. 2. ed. Springer, 2021.
- Se√ß√£o 6.2: The Lasso

---

## Conclus√£o

Lasso √© a escolha ideal quando:

‚úÖ **Precisa selecionar vari√°veis** importantes  
‚úÖ **Tem muitas vari√°veis** potenciais  
‚úÖ **Quer modelo simples** e interpret√°vel  
‚úÖ **Custo de coleta** de vari√°veis √© importante  
‚úÖ **Esparsidade** √© desej√°vel

**Lembre-se:**
- Sempre padronize vari√°veis
- Use valida√ß√£o cruzada para Œ±
- Compare com Ridge e Elastic Net
- Considere Elastic Net se vari√°veis correlacionadas
- An√°lise de estabilidade da sele√ß√£o √© importante

**Pr√≥ximos Passos:**
- Aprenda Elastic Net
- Explore m√©todos ensemble para sele√ß√£o de vari√°veis
- Estude Stability Selection
- Considere Algoritmos Gen√©ticos para sele√ß√£o

---

*Documento criado como parte do reposit√≥rio de An√°lise de Dados*  
*√öltima atualiza√ß√£o: 2026*
