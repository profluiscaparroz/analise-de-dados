# **An√°lise de Regress√£o: Fundamentos e Aplica√ß√µes**

A **an√°lise de regress√£o** √© uma t√©cnica estat√≠stica fundamental que investiga a rela√ß√£o entre vari√°veis, permitindo modelar e predizer o comportamento de uma vari√°vel dependente a partir de uma ou mais vari√°veis independentes.

## Sum√°rio

1. [Conceitos Fundamentais](#conceitos-fundamentais)
2. [Regress√£o Linear Simples](#regress√£o-linear-simples)
3. [Regress√£o Linear M√∫ltipla](#regress√£o-linear-m√∫ltipla)
4. [Pressupostos da Regress√£o](#pressupostos-da-regress√£o)
5. [Avalia√ß√£o do Modelo](#avalia√ß√£o-do-modelo)
6. [Diagn√≥stico e Valida√ß√£o](#diagn√≥stico-e-valida√ß√£o)
7. [Tipos Especiais de Regress√£o](#tipos-especiais-de-regress√£o)
8. [Exemplos Pr√°ticos](#exemplos-pr√°ticos)

---

## **Conceitos Fundamentais**

### **O que √© An√°lise de Regress√£o?**

A an√°lise de regress√£o √© uma metodologia estat√≠stica que:
- **Modela relacionamentos** entre vari√°veis
- **Faz predi√ß√µes** sobre valores futuros
- **Quantifica influ√™ncias** de vari√°veis explicativas
- **Identifica tend√™ncias** e padr√µes nos dados

### **Terminologia B√°sica**

- **Vari√°vel Dependente (Y)**: Vari√°vel que queremos explicar ou predizer
- **Vari√°vel Independente (X)**: Vari√°vel(is) que usamos para explicar Y
- **Intercepto (Œ≤‚ÇÄ)**: Valor de Y quando X = 0
- **Coeficiente Angular (Œ≤‚ÇÅ)**: Mudan√ßa em Y para cada unidade de mudan√ßa em X
- **Res√≠duos (Œµ)**: Diferen√ßa entre valores observados e preditos

### **Tipos de Regress√£o por Objetivo**

1. **Descritiva**: Descrever rela√ß√µes entre vari√°veis
2. **Preditiva**: Fazer previs√µes sobre novos dados
3. **Inferencial**: Testar hip√≥teses sobre rela√ß√µes

---

## **Regress√£o Linear Simples**

### **Modelo Matem√°tico**

O modelo de regress√£o linear simples √© expresso por:

$$Y = \beta_0 + \beta_1 X + \varepsilon$$

Onde:
- **Y**: Vari√°vel dependente
- **X**: Vari√°vel independente
- **Œ≤‚ÇÄ**: Intercepto (constante)
- **Œ≤‚ÇÅ**: Coeficiente angular (inclina√ß√£o)
- **Œµ**: Termo de erro aleat√≥rio

### **Interpreta√ß√£o dos Coeficientes**

- **Œ≤‚ÇÄ**: Valor esperado de Y quando X = 0
- **Œ≤‚ÇÅ**: Mudan√ßa esperada em Y para cada aumento de 1 unidade em X

### **Exemplo Pr√°tico: Vendas vs Temperatura**

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import seaborn as sns

# Dados simulados: Vendas de sorvete vs Temperatura
np.random.seed(42)
temperatura = np.random.normal(25, 5, 50)  # Temperatura m√©dia 25¬∞C
vendas = 100 + 3 * temperatura + np.random.normal(0, 10, 50)  # Vendas com ru√≠do

# Criar DataFrame
df = pd.DataFrame({
    'Temperatura': temperatura,
    'Vendas': vendas
})

print("Primeiras 5 observa√ß√µes:")
print(df.head())

# Visualiza√ß√£o
plt.figure(figsize=(10, 6))
plt.scatter(df['Temperatura'], df['Vendas'], alpha=0.6)
plt.xlabel('Temperatura (¬∞C)')
plt.ylabel('Vendas de Sorvete (R$)')
plt.title('Rela√ß√£o entre Temperatura e Vendas de Sorvete')
plt.grid(True, alpha=0.3)
plt.show()

# Ajustar modelo de regress√£o
X = df[['Temperatura']]
y = df['Vendas']

modelo = LinearRegression()
modelo.fit(X, y)

# Predi√ß√µes
y_pred = modelo.predict(X)

# Coeficientes
print(f"\nIntercepto (Œ≤‚ÇÄ): {modelo.intercept_:.2f}")
print(f"Coeficiente Angular (Œ≤‚ÇÅ): {modelo.coef_[0]:.2f}")
print(f"R¬≤: {r2_score(y, y_pred):.3f}")

# Interpreta√ß√£o
print(f"\nInterpreta√ß√£o:")
print(f"Para cada 1¬∞C de aumento na temperatura, as vendas aumentam em R$ {modelo.coef_[0]:.2f}")
print(f"Quando a temperatura √© 0¬∞C, as vendas esperadas s√£o R$ {modelo.intercept_:.2f}")
```

### **Linha de Regress√£o**

```python
# Plotar linha de regress√£o
plt.figure(figsize=(10, 6))
plt.scatter(df['Temperatura'], df['Vendas'], alpha=0.6, label='Dados observados')
plt.plot(df['Temperatura'], y_pred, color='red', linewidth=2, label='Linha de regress√£o')
plt.xlabel('Temperatura (¬∞C)')
plt.ylabel('Vendas de Sorvete (R$)')
plt.title('Regress√£o Linear: Vendas vs Temperatura')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Equa√ß√£o da reta
print(f"Equa√ß√£o: Vendas = {modelo.intercept_:.2f} + {modelo.coef_[0]:.2f} √ó Temperatura")
```

---

## **Regress√£o Linear M√∫ltipla**

### **Modelo Matem√°tico**

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \varepsilon$$

### **Exemplo: Pre√ßo de Im√≥veis**

```python
# Dados simulados de im√≥veis
np.random.seed(42)
n_amostras = 100

area = np.random.normal(100, 30, n_amostras)  # √Årea em m¬≤
quartos = np.random.poisson(3, n_amostras)   # N√∫mero de quartos
idade = np.random.exponential(10, n_amostras)  # Idade do im√≥vel

# Pre√ßo baseado em m√∫ltiplas vari√°veis
preco = (2000 * area + 
         15000 * quartos - 
         1000 * idade + 
         np.random.normal(0, 20000, n_amostras))

# DataFrame
df_imoveis = pd.DataFrame({
    'Area': area,
    'Quartos': quartos,
    'Idade': idade,
    'Preco': preco
})

print("Estat√≠sticas descritivas:")
print(df_imoveis.describe())

# Matriz de correla√ß√£o
print("\nMatriz de correla√ß√£o:")
correlation_matrix = df_imoveis.corr()
print(correlation_matrix)

# Visualizar correla√ß√µes
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Matriz de Correla√ß√£o - Vari√°veis do Im√≥vel')
plt.show()
```

### **Ajuste do Modelo M√∫ltiplo**

```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Vari√°veis independentes e dependente
X = df_imoveis[['Area', 'Quartos', 'Idade']]
y = df_imoveis['Preco']

# Divis√£o treino/teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ajustar modelo
modelo_multiplo = LinearRegression()
modelo_multiplo.fit(X_train, y_train)

# Predi√ß√µes
y_train_pred = modelo_multiplo.predict(X_train)
y_test_pred = modelo_multiplo.predict(X_test)

# Coeficientes
print("Coeficientes do modelo:")
print(f"Intercepto: R$ {modelo_multiplo.intercept_:,.2f}")
for i, coef in enumerate(modelo_multiplo.coef_):
    print(f"{X.columns[i]}: R$ {coef:,.2f}")

# M√©tricas de avalia√ß√£o
print(f"\nM√©tricas do modelo:")
print(f"R¬≤ (treino): {r2_score(y_train, y_train_pred):.3f}")
print(f"R¬≤ (teste): {r2_score(y_test, y_test_pred):.3f}")
print(f"RMSE (teste): R$ {np.sqrt(mean_squared_error(y_test, y_test_pred)):,.2f}")
print(f"MAE (teste): R$ {mean_absolute_error(y_test, y_test_pred):,.2f}")
```

---

## **Pressupostos da Regress√£o**

### **1. Linearidade**
A rela√ß√£o entre X e Y deve ser linear.

```python
# Verificar linearidade
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for i, col in enumerate(['Area', 'Quartos', 'Idade']):
    axes[i].scatter(df_imoveis[col], df_imoveis['Preco'], alpha=0.6)
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Pre√ßo')
    axes[i].set_title(f'Pre√ßo vs {col}')
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### **2. Independ√™ncia dos Res√≠duos**
Os res√≠duos devem ser independentes entre si.

### **3. Homocedasticidade**
A vari√¢ncia dos res√≠duos deve ser constante.

```python
# An√°lise de res√≠duos
residuos = y_test - y_test_pred

# Gr√°fico de res√≠duos vs valores preditos
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(y_test_pred, residuos, alpha=0.6)
plt.xlabel('Valores Preditos')
plt.ylabel('Res√≠duos')
plt.title('Res√≠duos vs Valores Preditos')
plt.axhline(y=0, color='red', linestyle='--')
plt.grid(True, alpha=0.3)

# Histograma dos res√≠duos
plt.subplot(1, 2, 2)
plt.hist(residuos, bins=15, alpha=0.7, edgecolor='black')
plt.xlabel('Res√≠duos')
plt.ylabel('Frequ√™ncia')
plt.title('Distribui√ß√£o dos Res√≠duos')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### **4. Normalidade dos Res√≠duos**

```python
from scipy import stats

# Teste de normalidade
shapiro_stat, shapiro_p = stats.shapiro(residuos)
print(f"Teste de Shapiro-Wilk:")
print(f"Estat√≠stica: {shapiro_stat:.4f}")
print(f"Valor-p: {shapiro_p:.4f}")

if shapiro_p > 0.05:
    print("Os res√≠duos seguem distribui√ß√£o normal (p > 0.05)")
else:
    print("Os res√≠duos n√£o seguem distribui√ß√£o normal (p ‚â§ 0.05)")

# Q-Q plot
from scipy.stats import probplot

plt.figure(figsize=(8, 6))
probplot(residuos, dist="norm", plot=plt)
plt.title('Q-Q Plot - Normalidade dos Res√≠duos')
plt.grid(True, alpha=0.3)
plt.show()
```

---

## **Avalia√ß√£o do Modelo**

### **M√©tricas de Avalia√ß√£o**

#### **1. Coeficiente de Determina√ß√£o (R¬≤)**
Propor√ß√£o da variabilidade explicada pelo modelo.

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$

```python
def metricas_regressao(y_true, y_pred):
    """Calcular m√∫ltiplas m√©tricas de regress√£o"""
    
    # R¬≤
    r2 = r2_score(y_true, y_pred)
    
    # R¬≤ ajustado
    n = len(y_true)
    p = X_test.shape[1]  # n√∫mero de vari√°veis
    r2_adj = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    
    # RMSE (Root Mean Squared Error)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    
    # MAE (Mean Absolute Error)
    mae = mean_absolute_error(y_true, y_pred)
    
    # MAPE (Mean Absolute Percentage Error)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    
    return {
        'R¬≤': r2,
        'R¬≤ Ajustado': r2_adj,
        'RMSE': rmse,
        'MAE': mae,
        'MAPE': mape
    }

# Calcular m√©tricas
metricas = metricas_regressao(y_test, y_test_pred)

print("M√©tricas de Avalia√ß√£o:")
for metrica, valor in metricas.items():
    if metrica in ['RMSE', 'MAE']:
        print(f"{metrica}: R$ {valor:,.2f}")
    elif metrica == 'MAPE':
        print(f"{metrica}: {valor:.2f}%")
    else:
        print(f"{metrica}: {valor:.3f}")
```

### **2. An√°lise de Import√¢ncia das Vari√°veis**

```python
# Import√¢ncia das vari√°veis (coeficientes padronizados)
from sklearn.preprocessing import StandardScaler

# Padronizar vari√°veis
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Ajustar modelo com dados padronizados
modelo_padronizado = LinearRegression()
modelo_padronizado.fit(X_scaled, y)

# Import√¢ncia relativa
importancia = pd.DataFrame({
    'Variavel': X.columns,
    'Coeficiente': modelo_multiplo.coef_,
    'Coef_Padronizado': modelo_padronizado.coef_,
    'Importancia_Abs': np.abs(modelo_padronizado.coef_)
}).sort_values('Importancia_Abs', ascending=False)

print("\nImport√¢ncia das Vari√°veis:")
print(importancia)

# Visualizar import√¢ncia
plt.figure(figsize=(10, 6))
plt.barh(importancia['Variavel'], importancia['Importancia_Abs'])
plt.xlabel('Import√¢ncia Absoluta (Coeficiente Padronizado)')
plt.title('Import√¢ncia das Vari√°veis no Modelo')
plt.grid(True, alpha=0.3)
plt.show()
```

---

## **Tipos Especiais de Regress√£o**

### **1. Regress√£o Polinomial**

```python
from sklearn.preprocessing import PolynomialFeatures

# Exemplo: Rela√ß√£o n√£o-linear entre idade do carro e deprecia√ß√£o
np.random.seed(42)
idade_carro = np.random.uniform(0, 20, 100)
valor_carro = 50000 * np.exp(-0.15 * idade_carro) + np.random.normal(0, 2000, 100)

# Regress√£o linear simples
modelo_linear = LinearRegression()
modelo_linear.fit(idade_carro.reshape(-1, 1), valor_carro)

# Regress√£o polinomial (grau 2)
poly_features = PolynomialFeatures(degree=2)
idade_poly = poly_features.fit_transform(idade_carro.reshape(-1, 1))

modelo_poly = LinearRegression()
modelo_poly.fit(idade_poly, valor_carro)

# Comparar modelos
idade_teste = np.linspace(0, 20, 100).reshape(-1, 1)
pred_linear = modelo_linear.predict(idade_teste)
pred_poly = modelo_poly.predict(poly_features.transform(idade_teste))

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(idade_carro, valor_carro, alpha=0.6, label='Dados')
plt.plot(idade_teste, pred_linear, 'r-', label='Linear')
plt.xlabel('Idade do Carro (anos)')
plt.ylabel('Valor (R$)')
plt.title('Regress√£o Linear')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(idade_carro, valor_carro, alpha=0.6, label='Dados')
plt.plot(idade_teste, pred_poly, 'g-', label='Polinomial (grau 2)')
plt.xlabel('Idade do Carro (anos)')
plt.ylabel('Valor (R$)')
plt.title('Regress√£o Polinomial')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Comparar R¬≤
r2_linear = r2_score(valor_carro, modelo_linear.predict(idade_carro.reshape(-1, 1)))
r2_poly = r2_score(valor_carro, modelo_poly.predict(idade_poly))

print(f"R¬≤ Linear: {r2_linear:.3f}")
print(f"R¬≤ Polinomial: {r2_poly:.3f}")
```

### **2. Regress√£o Ridge (L2)**

```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# Regress√£o Ridge para lidar com multicolinearidade
alphas = [0.1, 1, 10, 100]
ridge_scores = []

for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    scores = cross_val_score(ridge, X_train, y_train, cv=5, scoring='r2')
    ridge_scores.append(scores.mean())

# Melhor alpha
melhor_alpha = alphas[np.argmax(ridge_scores)]
print(f"Melhor alpha para Ridge: {melhor_alpha}")

# Modelo Ridge otimizado
ridge_modelo = Ridge(alpha=melhor_alpha)
ridge_modelo.fit(X_train, y_train)
ridge_pred = ridge_modelo.predict(X_test)

print(f"R¬≤ Ridge: {r2_score(y_test, ridge_pred):.3f}")
print(f"R¬≤ Linear: {r2_score(y_test, y_test_pred):.3f}")
```

### **3. Regress√£o Lasso (L1)**

```python
from sklearn.linear_model import Lasso

# Regress√£o Lasso para sele√ß√£o de vari√°veis
lasso = Lasso(alpha=1000)
lasso.fit(X_train, y_train)
lasso_pred = lasso.predict(X_test)

print(f"\nCoeficientes Lasso:")
for i, coef in enumerate(lasso.coef_):
    print(f"{X.columns[i]}: {coef:.2f}")

print(f"\nR¬≤ Lasso: {r2_score(y_test, lasso_pred):.3f}")
```

---

## **Exemplo Pr√°tico Completo: An√°lise de Vendas**

```python
# Cen√°rio: Empresa quer prever vendas baseado em investimento em marketing
np.random.seed(42)
n = 200

# Vari√°veis independentes
tv_ads = np.random.uniform(0, 300, n)          # Gastos com TV (milhares)
radio_ads = np.random.uniform(0, 50, n)       # Gastos com R√°dio (milhares)
newspaper_ads = np.random.uniform(0, 100, n)  # Gastos com Jornal (milhares)
competitor_price = np.random.uniform(50, 150, n)  # Pre√ßo do concorrente

# Vendas (vari√°vel dependente)
vendas = (2.5 * tv_ads + 
          4.0 * radio_ads + 
          0.1 * newspaper_ads +
          -0.8 * competitor_price +
          np.random.normal(0, 20, n) + 100)

# DataFrame
df_vendas = pd.DataFrame({
    'TV_Ads': tv_ads,
    'Radio_Ads': radio_ads,
    'Newspaper_Ads': newspaper_ads,
    'Competitor_Price': competitor_price,
    'Sales': vendas
})

print("Dataset de Vendas - Primeiras 5 linhas:")
print(df_vendas.head())

# An√°lise explorat√≥ria
print("\nCorrela√ß√£o com vendas:")
correlacoes = df_vendas.corr()['Sales'].sort_values(ascending=False)
print(correlacoes)

# Modelo final
X = df_vendas.drop('Sales', axis=1)
y = df_vendas['Sales']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

modelo_final = LinearRegression()
modelo_final.fit(X_train, y_train)
y_pred_final = modelo_final.predict(X_test)

# Resultados
print(f"\nResultados do Modelo de Vendas:")
print(f"R¬≤: {r2_score(y_test, y_pred_final):.3f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_final)):.2f}")

print(f"\nCoeficientes:")
print(f"Intercepto: {modelo_final.intercept_:.2f}")
for i, coef in enumerate(modelo_final.coef_):
    print(f"{X.columns[i]}: {coef:.2f}")

# Interpreta√ß√£o de neg√≥cio
print(f"\nInterpreta√ß√£o de Neg√≥cio:")
print(f"‚Ä¢ Para cada R$ 1.000 investidos em TV, vendas aumentam em {modelo_final.coef_[0]:.1f} unidades")
print(f"‚Ä¢ Para cada R$ 1.000 investidos em R√°dio, vendas aumentam em {modelo_final.coef_[1]:.1f} unidades")
print(f"‚Ä¢ Para cada R$ 1 de aumento no pre√ßo do concorrente, vendas aumentam em {modelo_final.coef_[3]:.1f} unidades")
```

---

## **Conclus√£o**

A an√°lise de regress√£o √© uma ferramenta poderosa para:

### **‚úÖ Aplica√ß√µes Pr√°ticas**
- **Previs√£o de vendas** baseada em investimento em marketing
- **Avalia√ß√£o de im√≥veis** considerando caracter√≠sticas f√≠sicas
- **An√°lise de fatores** que influenciam performance
- **Modelagem de crescimento** de neg√≥cios

### **üéØ Pontos-Chave**
- Sempre verificar pressupostos antes da interpreta√ß√£o
- R¬≤ alto n√£o garante modelo √∫til
- Avaliar signific√¢ncia pr√°tica al√©m da estat√≠stica
- Considerar overfitting em modelos complexos

### **‚ö†Ô∏è Limita√ß√µes**
- Assume rela√ß√µes lineares (exceto quando modificada)
- Sens√≠vel a outliers
- Pressupostos devem ser atendidos
- Correla√ß√£o ‚â† Causa√ß√£o

### **üöÄ Pr√≥ximos Passos**
- Explorar m√©todos de regulariza√ß√£o (Ridge, Lasso)
- Aprender regress√£o log√≠stica para vari√°veis categ√≥ricas
- Estudar m√©todos n√£o-lineares (Random Forest, Neural Networks)
- Implementar valida√ß√£o cruzada e sele√ß√£o de modelos

---

## **Refer√™ncias**

**JAMES, Gareth et al.** *An Introduction to Statistical Learning: with Applications in R*. 2. ed. New York: Springer, 2021.

**MONTGOMERY, Douglas C.; PECK, Elizabeth A.; VINING, G. Geoffrey.** *Introduction to Linear Regression Analysis*. 5. ed. Hoboken: Wiley, 2012.

**KUTNER, Michael H. et al.** *Applied Linear Statistical Models*. 5. ed. New York: McGraw-Hill, 2005.

**HASTIE, Trevor; TIBSHIRANI, Robert; FRIEDMAN, Jerome.** *The Elements of Statistical Learning*. 2. ed. New York: Springer, 2009.